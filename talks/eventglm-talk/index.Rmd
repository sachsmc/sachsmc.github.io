---
title: "Recent advances in regression modeling of censored time-to-event outcomes using pseudo-observations"
author: "Michael Sachs, Erin Gabriel"
date: "2022-06-09"
output:
  xaringan::moon_reader:
    css: ["default", "ki.css", "ki-fonts.css"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: '16:9'
    #self_contained: true
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
library(knitr)
library(eventglm)
opts_chunk$set(echo = FALSE, fig.width = 10, fig.height = 10 * .618, 
               fig.align = "center")

```

class: inverse, center, middle

# Background

---

# Applied project

Title: "Hepatobiliary Cancer Risk in Patients with Inflammatory Bowel Disease: A Scandinavian Population-Based Cohort Study"

### Motivation: 

- People with IBD tend to get liver cancer more than those without
- Liver cancer is quite rare, and most previous studies report hazard ratios
- "You are at 5 times higher risk of liver cancer" sounds scary, but should we worry enough to, e.g., do liver cancer screening? 
- Instead let's estimate absolute risks and adjusted risk differences

---

# What does "risk" mean? 

"The possibility of suffering harm or loss" -Regular dictionary

"The probability that an event will occur, e.g., that an individual will become ill or die within a stated period" -A Dictionary of Epidemiology

If having an earlier event is bad (e.g., death), then this is best quantified with $P(T < t)$ and contrasted based on $P(T < t | X = x)$. 

Compared to the hazard $$\lim_{\delta \rightarrow 0} \frac{P(t \leq T < t + \delta | T \geq t)}{\delta}$$

the risk is 

- cleaner to interpret, esp. as a causal quantity
- has some nicer statistical properties (e.g., collapsibility)
- more precise ("within a stated period")


---

# Some details

- Danish-Swedish collaboration -- Register-based study involving all individuals diagnosed with IBD since 1969 and 10 matched controls per case
- Main outcomes of interest: death due to HCC, ICC, and ECC (different types of liver cancer)
- Main exposure of interest: IBD, also looking in a list of about 20 subgroups (demographics, IBD features, comorbidities)

---


# Translating to the statistical approach

- Statistic of interest is the probability of death due to liver cancer
    + Death from other causes considered a competing event
- Want to estimate risk differences adjusted for matching variables (age, sex, place of residence)
    + chronic hepatitis, alcohol-related diseases (other than liver cirrhosis), diabetes, and chronic obstructive pulmonary disease
        
*Sounds like a job for pseudo-observations!* ðŸ¦¸

---

class: middle, center, inverse

# A *brief* review of pseudo-observations

---

# Notation

Let $T_i$ denote the time to event, $\delta_i \in \{1, \ldots, d\}$
denote the indicator of the cause of the event for $d$ competing
causes, and $X_i$ a vector of covariates for subject $i = 1, \ldots, n$.  

For a fixed $t$, and for $V_i = 1\{T_i < t, \delta_i=k\}$, We would like to fit the model 

$$
E(V_i | X_i) = P(T_i < t, \delta_i = k | X_i) = \beta^\top X_i.
$$

We do not observe $T_i$ and $\delta_i$ directly, but rather $Y_i = \min\{C_i, T_i\}$ where $C_i$ is the censoring time, and $\Delta_i \in \{0, 1, \ldots, d\}$ where where 0 indicates censoring occurred before any of the events.

---

# Estimating equations

*If* the pseudo-observations denoted by $P_i$ satisfy 

$$E(P_i | X_i) = E(V_i | X_i) + o_p(1)$$

in large samples, then solving the estimating equations

$$\sum_{i = 1}^n \frac{\partial g^{-1}}{\partial \beta} A_i^{-1} \{P_i - g^{-1}(X_i^{\top} \beta)\} = \sum_{i = 1}^n U_i(\beta) = 0,$$

for some specified variance parameter $A_i$ yields consistent estimates of $\beta$ in our model. 

Then the variance of $\hat{\beta}$ can be estimated using the sandwich variance estimator, or a more complicated one due to Overgaard (2017). 

---

# Obtaining the pseudo-observations

In general, 

$$P_i = n \hat{\theta} - (n - 1) \hat{\theta}_{-i} = \hat{\theta} + (n-1)(\hat{\theta} - \hat{\theta}_{-i}).$$

where $\hat{\theta}$ is an estimator of $E(V_i)$ using all $n$ observations, and $\hat{\theta}_{-i}$ is an estimator of the same thing leaving the $i$ th subject out. 

- Under completely independent censoring, we can use the Aalen-Johansen estimator
- If censoring depends only on a set of observed covariates with finite domain, we can use the AJ estimator conditional on those covariate values
- If censoring depends on a set of observed covariates according to the model $P(C_i > s | \tilde{X}_i) = G(s | \tilde{X}_i)$ then we can use an inverse probability of censoring weighted estimator
    + In practice we estimate $G$ using Aalen's additive model or a Cox model. 


---

class: inverse, center, middle

# Technical challenges

---

# Computation

Naively, computing the pseudo-observations requires fitting the marginal estimator $n + 1$ times
- Danish + Swedish IBD cases plus controls $\approx$ 1.65 million individuals. 
 
So the **pseudo** package wasn't feasible to use, I went looking for a faster implementation and found **prodlim**. 

**prodlim** implementation: 

- Written in C
- Avoids $n + 1$ estimates by using clever data structures and counting process representation
- Only a nested for loop through all distinct times and observations (after sorting)


This was theoretically fast enough, but crashed R due to running out of memory

---

# Small tweak to **prodlim**

.pull-left[
`loo_comprisk`
```c
for (k=0; k<*N;k++){ /* observations */
for (t=0; t<*Tdex;t++){ /* distinct times<t */
/* compute the NA estimate */
       .
       .
       .
ls *= (1-na0);
/* compute the AJ estimate */
aj += ls*na;

F[k+(*N)*t]=aj;
}}
```
]

.pull-right[
`loo_comprisk2`
```c
for (k=0; k<*N;k++){ /* observations */
for (t=0; t<*NT;t++){ /* distinct times */
/* compute the NA estimate */
       .
       .
       .
ls *= (1-na0);
/* compute the AJ estimate */
aj += ls*na;

if(t == *Tdex){
  F[k]=aj;
}
}
```
]

---

# Interface

- Since I needed to fit around 60 different models for the main results, I needed a concise function to call that does the main analysis steps: 

1. Calculate the pseudo-observations
2. Fit the regression model
3. Compute the variance
4. Print out confidence intervals

---

class: center, middle, inverse

# **eventglm** package

---

# Basic usage

```{r, echo = TRUE, eval = FALSE}
library("eventglm")
data(colon)

colon.cifit <- cumincglm(Surv(time, status) ~ rx, 
                         time = 2500, data = colon)

# P(T > t) instead of <
cumincglm(Surv(time, status) ~ rx, 
          time = 2500, survival = TRUE,
          data = colon)

# any link supported by quasi 
cumincglm(Surv(time, status) ~ rx, 
          time = 2500, link = "log",
          data = colon)

# restricted mean survival
colon.rmfit <- rmeanglm(Surv(time, status) ~ rx, 
                         time = 2500, data = colon)
summary(colon.cifit)
```

---

# Competing risks

```{r, echo = TRUE, eval = FALSE}
data(mgus2)

# specify the cause of interest
mgfitci <- cumincglm(Surv(etime, event) ~ sex, 
                     cause = "pcm", time = 120, #<<
                   data = mgus2)

# restricted mean lifetime lost due to cause
mgfitrm <- rmeanglm(Surv(etime, event) ~ sex, 
                     cause = "pcm", time = 120, #<<
                   data = mgus2)

summary(mgfitci)
```

---

# Multiple time points

Consider a finite set of time points $t_1, \ldots, t_k$, and the models of the form

$$g\{P(T_i < t_b| X_i = x_i)\} = (\beta_0 + \beta_b) + \beta_1x_i, b = 1, \ldots, k$$

```{r, echo = TRUE}
mvtfit1 <- cumincglm(Surv(time, status) ~ rx, 
        time = c(500, 1000, 1500, 2000, 2500),
        data = colon)
```

These models are estimated using GEE with working independence

---

```{r}
summary(mvtfit1)
```

---

# Time varying effects


$$g\{P(T_i < t_b| X_i = x_i)\} = (\beta_0 + \beta_b) + \gamma_b x_i, b = 1, \ldots, k$$


```{r, echo = TRUE}
# can mix and match time varying and time constant effects
mvtfit2 <- cumincglm(Surv(time, status) ~ tve(rx), 
        time = c(500, 1000, 1500, 2000, 2500),
        data = colon)
```

---

```{r}
mvtfit2
```

---

# Censoring assumptions (1)

```{r, echo = TRUE, eval = FALSE}
cumincglm(Surv(time, status) ~ rx + age, time = 2500, 
          model.censoring = "independent", data = colon)

cumincglm(Surv(time, status) ~ rx + age, time = 2500, 
          model.censoring = "stratified", 
          formula.censoring = ~ rx, data = colon)
```

The above estimated using the AJ estimator. Restricted mean based on sums of the jackknifed AJ.

---

# Censoring assumptions (2)

```{r, echo = TRUE, eval = FALSE}
cumincglm(Surv(time, status) ~ rx + age, time = 2500, 
          model.censoring = "coxph", 
          formula.censoring = ~ rx + age, data = colon)

cumincglm(Surv(time, status) ~ rx + age, time = 2500, 
          model.censoring = "aareg", 
          formula.censoring = ~ rx + age + age^2, data = colon)

```

With $I_i = 1\{C_i \geq \min(T_i, t)\}$ and $\tilde{V}_i = 1\{T_i < t\}$ if $T_i < C_i$ and 0 otherwise, these are estimated using 

$$\hat{\theta}^b = n^{-1}\sum_{i = 1}^n \frac{\tilde{V}_iI_i}{\hat{G}\{\min(\tilde{T}_i, t^*); \tilde{X}_i\}} \mbox{ or } \hat{\theta}^h = \frac{\sum_{i = 1}^n \tilde{V}_iw_i}{\sum_{i = 1}^n w_i}$$

$\mbox{ where } w_i = \frac{I_i}{\hat{G}\{\min(\tilde{T}_i, t); \tilde{X}_i\}},$ with `ipcw.method = "binder"` (the default) or `ipcw.method = "hajek"`, respectively.

---

# pseudoglm class

```{r, echo = TRUE}
class(mvtfit1)
methods(class = "pseudoglm")
```

---

# Variance estimation

Default in `vcov` is `type = "robust"` which uses `sandwich::vcovHC`. Other options: 

- "corrected": Overgaard's variance estimator. Does not handle ties, only works for cumulative incidence at a single time point
- "naive": Not recommended
- "cluster": uses `sandwich::vcovCL`

Bootstrap works well, but there is no special function/option for that

Approximate jackknife (geepack) potentially an option, as recommended by Klein et al. (2008).

---

# Other details

- The computed pseudo-observations are returned in the `y` element of any object of class `pseudoglm`. They can be used for other fun things, e.g., estimating relative survival as per PavliÄ and Pohar Perme (2019)
- IPCWs are returned in the `ipcw.weights` element
- Residuals for the cumulative incidence are scaled using the knowledge that they are probabilities, as per Pohar-Perme and Andersen (2009)
- The `weights` argument can be used for sampling weights, e.g., case-cohort sampling as per Parner, Andersen, and Overgaard (2020). 

---

# The package in practice

Rossides, et al. (2018): "After adjusting for relevant confounders, individuals with sarcoidosis had a 62% higher risk for all-cause death compared to the general population.", based on a single estimated hazard ratio assumed to be constant over the entire follow-up period. We reanalyzed these data using our package, entries are relative risks: 

year |overall	|untreated	|treated 
----|-------|------------|-------
2	|1.70 (1.57 to 1.83)	|1.18 (0.83 to 1.67)|	1.99 (1.38 to 2.86)
3	|1.41 (1.17 to 1.70)	|1.32 (1.05 to 1.67)|	1.70 (1.26 to 2.29)
4	|1.43 (1.24 to 1.66)	|1.30 (1.05 to 1.60)|	1.77 (1.43 to 2.19)
5	|1.43 (1.25 to 1.62)	|1.32 (1.11 to 1.58)|	1.62 (1.30 to 2.00)
6	|1.37 (1.22 to 1.55)	|1.23 (1.04 to 1.45)|	1.61 (1.31 to 1.99)
7	|1.34 (1.20 to 1.49)	|1.21 (1.03 to 1.43)|	1.56 (1.26 to 1.92)
8	|1.34 (1.20 to 1.49)	|1.17 (0.99 to 1.38)|	1.45 (1.17 to 1.81)
9	|1.37 (1.23 to 1.52)	|NA	|NA 
10	|1.30 (1.17 to 1.44)	|NA	|NA 


---

class: inverse, center, middle

# Extending **eventglm**

---

# Modules for computing pseudo-observations

The `model.censoring` argument is a function, passed by name or directly. The package includes modules for independent, stratified, coxph, and aareg. It must take the same form as this example: 

```{r, echo = TRUE, eval = FALSE}
function(formula, time, cause = 1, data,
                        type = c("cuminc", "survival", "rmean"),
                        formula.censoring = NULL, ipcw.method = NULL){

  ## do something
    ...
    
  ## return pseudo observations
  POi
  
}
```

---

```{r, echo = TRUE, eval = FALSE}
pseudo_parametric <- function(formula, time, cause = 1, data,
                        type = c("cuminc", "survival", "rmean"),
                        formula.censoring = NULL, ipcw.method = NULL){

  margformula <- update.formula(formula, . ~ 1)
  mr <- model.response(model.frame(margformula, data = data))
  
  marginal.estimate <- survival::survreg(margformula, data = data, #<<
                                         dist = "weibull") #<<

  theta <- pweibull(time, shape = 1 / marginal.estimate$scale, 
                    scale = exp(marginal.estimate$coefficients[1]))
  
  theta.i <- sapply(1:nrow(data), function(i) {
    me <- survival::survreg(margformula, data = data[-i, ], dist = "weibull") #<<
    pweibull(time, shape = 1 / me$scale, 
                    scale = exp(me$coefficients[1]))
  })
  
  POi <- theta  + (nrow(data) - 1) * (theta - theta.i)
  POi

}
```

---

# Infinitesimal jackknife

Let $T(P)$ denote the functional of interest (e.g., the Aalen-Johansen functional) and $T(\mathbb{P}_n)$ its empirical counterpart, where $\mathbb{P}_n = n^{-1}\sum_{i=1}^n \delta(x_i)$ is the empirical measure based on $n$ iid observations.

.pull-left[
The jackknife pseudo-observations can be written 

$$T(\mathbb{P}_n) + (n - 1) (T(\mathbb{P}_n) - T(\mathbb{P}_{ni})) =$$
$$T(\mathbb{P}_n) + (n - 1) (T(\mathbb{P}_n) - T\left(\frac{n\mathbb{P}_{n} - \delta(x_i)}{n-1}\right))$$

where $\mathbb{P}_{ni}$ is the empirical measure leaving out the $i$th observation

]

.pull-right[
The influence function of $T$ evaluated at $P$ is $$\phi_P(x) = \partial T(P - \delta(x))$$ is a functional derivative, i.e., an approximation of the original functional. 

$$\phi_P(x) \approx \lim_{\epsilon \rightarrow 0}\frac{T(P) - T(P + \epsilon(\delta(x) - P))}{\epsilon}$$
]

If we can calculate this, then $\phi_{\mathbb{P}_n}(x_i)$ gives us the perturbation in the estimate infinitesimally close to $x_i$, whereas the ordinary jackknife is $1/n$ units away. See Efron (1982), Chapter 6. 

These are computed already in `survival` for variance estimation

---

# Another custom module

```{r, echo = TRUE, eval = FALSE}
pseudo_infjack <- function(formula, time, cause = 1, data,
                        type = c("cuminc", "survival", "rmean"),
                        formula.censoring = NULL, ipcw.method = NULL) {
  
  marginal.estimate2 <- survival::survfit(update.formula(formula, . ~ 1),
                                             data = data, influence = TRUE) #<<

     tdex <- sapply(time, function(x) 
         max(which(marginal.estimate2$time <= x)))
     pstate <- marginal.estimate2$surv[tdex]
     ## S(t) + (n)[S(t) -S_{-i}(t)]
     POi <- matrix(pstate, nrow = marginal.estimate2$n, 
                   ncol = length(time), byrow = TRUE) +
         (marginal.estimate2$n) *
         (marginal.estimate2$influence.surv[, tdex]) #<<
     
     POi
}
```


---

# More general models

- Since `survfit` can be used to fit general multi-state models, this infinitesimal jackknife opens up lots of new possibilities. 
- Let $E_i$ denote the entry time into the study, this could be determined by, e.g., some other event occuring, if we are studying the time from complete response to death. 
- In addition to the other assumptions regarding right censoring, we need the entry time to be completely independent for the pseudo observation method to work. 

Then it is as easy as specifying the start time in the call to `Surv`: 

```{r echo = TRUE, eval = FALSE}
tinf <- cumincglm(Surv(tstart, tstop, death) ~ trt, data = mdata,
          time = 750, model.censoring = "infjack")
```

In a more general multi-state model, a similar approach can be implemented to model cumulative quantities in the model the probability of being in a state at a fixed time, or the expected length of stay in a state up to a fixed time. 

---

# Simultaneous inference

- In multi-state models, it is often of interest to test for exposure effects on multiple outcomes simultaneously, e.g., does treatment reduce time to recurrence or death? 
- Furberg et al. 2021, developed the idea of specifying two models, one for each outcome type, then stacking the estimating equations to simultaneously estimate covariate effects on different outcomes, say $(\hat{\beta}, \hat{\gamma})$ and their covariance $\hat{\Sigma}$.
- Then one can do the joint test using the Wald statistic

$$
(\hat{\beta}, \hat{\gamma})^\top \hat{\Sigma}^{-1} (\hat{\beta}, \hat{\gamma})
$$

- They also develop sequential tests, and hierarchical tests motivated by trials of treatments for cardiovascular disease. 
- Noninferiority for death, superiority for recurrent heart attack events


---

# Conclusion

- Goal was to make something easy to use, efficient, but flexible enough to be useful in practice
- We've purposefully de-emphasized the fact that pseudo-observations are used for estimation
- Modular approach will
    + make maintenance easier
    + enable more estimands (unrestricted mean), 
    + flexibility (more general multi-state models). 
    + novel approaches (influence functions) 
    

---

# For more details

Documentation and vignettes: https://sachsmc.github.io/eventglm/

Tutorial this afternoon! https://sachsmc.github.io/eventglm-tutorial/

Sachs MC, Gabriel EE (2022). â€œEvent History Regression with Pseudo-Observations: Computational
Approaches and an Implementation in R.â€ _Journal of Statistical Software_, *102*(9), 1-34.

Gabriel EE, Arkema EV, Sachs MC. Direct modeling of relative and absolute risks in register data: mortality risk in sarcoidosis. _Annals of Epidemiology_. 2021 Nov 11.


---

class: inverse, center, middle

# Thank you!
